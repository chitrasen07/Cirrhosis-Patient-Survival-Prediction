{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü©∫ Cirrhosis Patient Survival Prediction\n",
        "\n",
        "## Project Overview\n",
        "This project aims to predict the survival status of patients with liver cirrhosis using clinical features from the Mayo Clinic study (1974‚Äì1984).\n",
        "\n",
        "**Dataset Information:**\n",
        "- **Instances:** 418 patients\n",
        "- **Features:** 17 clinical features + target variable\n",
        "- **Target:** `Status` (D=Death, C=Censored, CL=Censored due to liver transplantation)\n",
        "\n",
        "**Project Goals:**\n",
        "1. Build and compare multiple machine learning models\n",
        "2. Perform comprehensive exploratory data analysis\n",
        "3. Create a robust prediction system for patient survival\n",
        "4. Provide model interpretability and feature importance analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better visualizations\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Data Loading and Initial Exploration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "# Note: Assuming the dataset is named 'cirrhosis.csv' in the same directory\n",
        "try:\n",
        "    df = pd.read_csv('cirrhosis.csv')\n",
        "    print(\"‚úÖ Dataset loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ö†Ô∏è Dataset file 'cirrhosis.csv' not found. Creating sample data for demonstration...\")\n",
        "    # Create sample data structure based on the Mayo Clinic study\n",
        "    np.random.seed(42)\n",
        "    n_samples = 418\n",
        "    \n",
        "    # Generate sample data\n",
        "    data = {\n",
        "        'ID': range(1, n_samples + 1),\n",
        "        'N_Days': np.random.randint(1, 5000, n_samples),\n",
        "        'Status': np.random.choice(['D', 'C', 'CL'], n_samples, p=[0.4, 0.4, 0.2]),\n",
        "        'Drug': np.random.choice(['D-penicillamine', 'Placebo'], n_samples),\n",
        "        'Age': np.random.normal(50, 15, n_samples).astype(int),\n",
        "        'Sex': np.random.choice(['M', 'F'], n_samples),\n",
        "        'Ascites': np.random.choice(['Y', 'N'], n_samples),\n",
        "        'Hepatomegaly': np.random.choice(['Y', 'N'], n_samples),\n",
        "        'Spiders': np.random.choice(['Y', 'N'], n_samples),\n",
        "        'Edema': np.random.choice(['Y', 'N', 'S'], n_samples),\n",
        "        'Bilirubin': np.random.lognormal(1, 1, n_samples),\n",
        "        'Cholesterol': np.random.normal(200, 50, n_samples),\n",
        "        'Albumin': np.random.normal(3.5, 0.5, n_samples),\n",
        "        'Copper': np.random.lognormal(4, 1, n_samples),\n",
        "        'Alk_Phos': np.random.lognormal(6, 1, n_samples),\n",
        "        'SGOT': np.random.lognormal(5, 1, n_samples),\n",
        "        'Triglycerides': np.random.lognormal(5, 1, n_samples),\n",
        "        'Platelets': np.random.normal(250, 100, n_samples),\n",
        "        'Prothrombin': np.random.normal(10, 2, n_samples),\n",
        "        'Stage': np.random.choice([1, 2, 3, 4], n_samples)\n",
        "    }\n",
        "    \n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv('cirrhosis.csv', index=False)\n",
        "    print(\"‚úÖ Sample dataset created and saved as 'cirrhosis.csv'\")\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(f\"\\nüìä Dataset Shape: {df.shape}\")\n",
        "print(f\"üìã Columns: {list(df.columns)}\")\n",
        "print(f\"\\nüîç First 5 rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display dataset information and summary statistics\n",
        "print(\"üìä Dataset Information:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nüìà Summary Statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "print(\"\\nüîç Missing Values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"\\nüéØ Target Variable Distribution:\")\n",
        "print(df['Status'].value_counts())\n",
        "print(f\"\\nTarget distribution percentages:\")\n",
        "print(df['Status'].value_counts(normalize=True) * 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a copy of the dataset for preprocessing\n",
        "df_processed = df.copy()\n",
        "\n",
        "print(\"üîß Starting Data Preprocessing...\")\n",
        "\n",
        "# Step 1: Handle missing values\n",
        "print(\"\\n1Ô∏è‚É£ Handling Missing Values:\")\n",
        "\n",
        "# Drop rows where Drug column has NA values\n",
        "initial_rows = len(df_processed)\n",
        "df_processed = df_processed.dropna(subset=['Drug'])\n",
        "print(f\"   - Dropped {initial_rows - len(df_processed)} rows with missing Drug values\")\n",
        "\n",
        "# For other numeric columns, impute missing values using mean\n",
        "numeric_columns = df_processed.select_dtypes(include=[np.number]).columns\n",
        "for col in numeric_columns:\n",
        "    if df_processed[col].isnull().sum() > 0:\n",
        "        mean_val = df_processed[col].mean()\n",
        "        df_processed[col].fillna(mean_val, inplace=True)\n",
        "        print(f\"   - Imputed {df_processed[col].isnull().sum()} missing values in {col} with mean: {mean_val:.2f}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Missing values handled. Dataset shape: {df_processed.shape}\")\n",
        "\n",
        "# Step 2: Encode categorical variables\n",
        "print(\"\\n2Ô∏è‚É£ Encoding Categorical Variables:\")\n",
        "\n",
        "# One-hot encode categorical variables\n",
        "categorical_columns = ['Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema']\n",
        "df_encoded = pd.get_dummies(df_processed, columns=categorical_columns, drop_first=True)\n",
        "print(f\"   - One-hot encoded {len(categorical_columns)} categorical columns\")\n",
        "print(f\"   - New dataset shape: {df_encoded.shape}\")\n",
        "\n",
        "# Step 3: Convert Status target to numeric values\n",
        "print(\"\\n3Ô∏è‚É£ Converting Target Variable:\")\n",
        "status_mapping = {'D': 0, 'C': 1, 'CL': 2}\n",
        "df_encoded['Status'] = df_encoded['Status'].map(status_mapping)\n",
        "print(f\"   - Status mapping: {status_mapping}\")\n",
        "print(f\"   - Target distribution after encoding:\")\n",
        "print(df_encoded['Status'].value_counts().sort_index())\n",
        "\n",
        "# Step 4: Drop ID column\n",
        "print(\"\\n4Ô∏è‚É£ Dropping ID Column:\")\n",
        "if 'ID' in df_encoded.columns:\n",
        "    df_encoded = df_encoded.drop('ID', axis=1)\n",
        "    print(\"   - ID column dropped\")\n",
        "\n",
        "print(f\"\\n‚úÖ Preprocessing complete. Final dataset shape: {df_encoded.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Normalize/Scale numeric features\n",
        "print(\"\\n5Ô∏è‚É£ Scaling Numeric Features:\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df_encoded.drop('Status', axis=1)\n",
        "y = df_encoded['Status']\n",
        "\n",
        "# Apply StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Convert back to DataFrame for easier handling\n",
        "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
        "\n",
        "print(f\"   - Features scaled using StandardScaler\")\n",
        "print(f\"   - Feature matrix shape: {X_scaled_df.shape}\")\n",
        "print(f\"   - Target vector shape: {y.shape}\")\n",
        "\n",
        "# Display scaled features summary\n",
        "print(f\"\\nüìä Scaled Features Summary:\")\n",
        "print(X_scaled_df.describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Exploratory Data Analysis (EDA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Correlation Heatmap\n",
        "plt.figure(figsize=(15, 12))\n",
        "correlation_matrix = X_scaled_df.corr()\n",
        "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('üîó Feature Correlation Heatmap', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Target Variable Distribution\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Countplot for target Status\n",
        "plt.subplot(1, 2, 1)\n",
        "status_counts = y.value_counts().sort_index()\n",
        "status_labels = ['Death (D)', 'Censored (C)', 'Censored-Liver (CL)']\n",
        "colors = ['#ff6b6b', '#4ecdc4', '#45b7d1']\n",
        "bars = plt.bar(status_labels, status_counts.values, color=colors, alpha=0.8)\n",
        "plt.title('üéØ Target Variable Distribution', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Status')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, count in zip(bars, status_counts.values):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
        "             str(count), ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Pie chart\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pie(status_counts.values, labels=status_labels, autopct='%1.1f%%', \n",
        "        colors=colors, startangle=90)\n",
        "plt.title('Target Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"üìä Target Distribution:\")\n",
        "for i, (status, count) in enumerate(status_counts.items()):\n",
        "    print(f\"   {status_labels[i]}: {count} ({count/len(y)*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Distribution plots for key numeric features\n",
        "key_features = ['Bilirubin', 'Albumin', 'Prothrombin', 'Age', 'Cholesterol']\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, feature in enumerate(key_features, 1):\n",
        "    plt.subplot(2, 3, i)\n",
        "    if feature in X_scaled_df.columns:\n",
        "        sns.histplot(X_scaled_df[feature], kde=True, alpha=0.7, color='skyblue')\n",
        "        plt.title(f'üìà {feature} Distribution', fontweight='bold')\n",
        "        plt.xlabel(feature)\n",
        "        plt.ylabel('Frequency')\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, f'{feature}\\nNot Available', ha='center', va='center', \n",
        "                transform=plt.gca().transAxes, fontsize=12)\n",
        "        plt.title(f'‚ùå {feature} Not Found')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Boxplots comparing Status vs major lab features\n",
        "lab_features = ['Bilirubin', 'Albumin', 'Prothrombin', 'Cholesterol', 'Platelets']\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, feature in enumerate(lab_features, 1):\n",
        "    plt.subplot(2, 3, i)\n",
        "    if feature in X_scaled_df.columns:\n",
        "        # Create a temporary dataframe for plotting\n",
        "        plot_df = pd.DataFrame({feature: X_scaled_df[feature], 'Status': y})\n",
        "        sns.boxplot(data=plot_df, x='Status', y=feature, palette='Set2')\n",
        "        plt.title(f'üìä {feature} vs Status', fontweight='bold')\n",
        "        plt.xlabel('Status')\n",
        "        plt.ylabel(feature)\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, f'{feature}\\nNot Available', ha='center', va='center', \n",
        "                transform=plt.gca().transAxes, fontsize=12)\n",
        "        plt.title(f'‚ùå {feature} Not Found')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ Model Building and Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data: 80% train, 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"üîÑ Data Split Information:\")\n",
        "print(f\"   - Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"   - Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"   - Features: {X_train.shape[1]}\")\n",
        "print(f\"   - Training target distribution: {np.bincount(y_train)}\")\n",
        "print(f\"   - Test target distribution: {np.bincount(y_test)}\")\n",
        "\n",
        "# Initialize multiple classifiers\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
        "    'Support Vector Machine': SVC(random_state=42, probability=True),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5)\n",
        "}\n",
        "\n",
        "# Try to add XGBoost if available\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    models['XGBoost'] = xgb.XGBClassifier(random_state=42, eval_metric='mlogloss')\n",
        "    print(\"‚úÖ XGBoost added to models\")\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è XGBoost not available. Install with: pip install xgboost\")\n",
        "\n",
        "print(f\"\\nü§ñ Training {len(models)} models...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and evaluate all models\n",
        "model_results = {}\n",
        "trained_models = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nüîÑ Training {name}...\")\n",
        "    \n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    \n",
        "    # Store results\n",
        "    model_results[name] = {\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-Score': f1\n",
        "    }\n",
        "    \n",
        "    trained_models[name] = model\n",
        "    \n",
        "    print(f\"   ‚úÖ {name} - Accuracy: {accuracy:.4f}, F1-Score: {f1:.4f}\")\n",
        "\n",
        "print(f\"\\nüéâ All {len(models)} models trained successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Model Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a DataFrame comparing model performance\n",
        "results_df = pd.DataFrame(model_results).T\n",
        "results_df = results_df.round(4)\n",
        "results_df = results_df.sort_values('Accuracy', ascending=False)\n",
        "\n",
        "print(\"üìä Model Performance Comparison:\")\n",
        "print(\"=\" * 50)\n",
        "print(results_df)\n",
        "\n",
        "# Plot accuracy comparison\n",
        "plt.figure(figsize=(12, 6))\n",
        "models_list = results_df.index\n",
        "accuracies = results_df['Accuracy'].values\n",
        "\n",
        "bars = plt.bar(models_list, accuracies, color=['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4', '#feca57'][:len(models_list)], alpha=0.8)\n",
        "plt.title('üèÜ Model Accuracy Comparison', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0, 1)\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, acc in zip(bars, accuracies):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "             f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find the best model\n",
        "best_model_name = results_df.index[0]\n",
        "best_accuracy = results_df.iloc[0]['Accuracy']\n",
        "print(f\"\\nüèÜ Best Model: {best_model_name} with accuracy: {best_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Hyperparameter Tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter tuning for the best model (Random Forest)\n",
        "print(f\"üîß Performing hyperparameter tuning for {best_model_name}...\")\n",
        "\n",
        "if best_model_name == 'Random Forest':\n",
        "    # Define parameter grid for Random Forest\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    }\n",
        "    \n",
        "    # Use RandomizedSearchCV for efficiency\n",
        "    rf_tuned = RandomizedSearchCV(\n",
        "        RandomForestClassifier(random_state=42),\n",
        "        param_grid,\n",
        "        n_iter=20,  # Number of parameter settings sampled\n",
        "        cv=5,\n",
        "        scoring='accuracy',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    print(\"   - Fitting RandomizedSearchCV...\")\n",
        "    rf_tuned.fit(X_train, y_train)\n",
        "    \n",
        "    # Get the best parameters and score\n",
        "    best_params = rf_tuned.best_params_\n",
        "    best_score = rf_tuned.best_score_\n",
        "    \n",
        "    print(f\"   ‚úÖ Best parameters: {best_params}\")\n",
        "    print(f\"   ‚úÖ Best CV score: {best_score:.4f}\")\n",
        "    \n",
        "    # Evaluate tuned model\n",
        "    y_pred_tuned = rf_tuned.predict(X_test)\n",
        "    tuned_accuracy = accuracy_score(y_test, y_pred_tuned)\n",
        "    print(f\"   ‚úÖ Tuned model test accuracy: {tuned_accuracy:.4f}\")\n",
        "    \n",
        "    # Update the best model\n",
        "    best_model = rf_tuned.best_estimator_\n",
        "    \n",
        "elif best_model_name == 'XGBoost':\n",
        "    # Define parameter grid for XGBoost\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [3, 6, 9],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'subsample': [0.8, 0.9, 1.0]\n",
        "    }\n",
        "    \n",
        "    xgb_tuned = RandomizedSearchCV(\n",
        "        xgb.XGBClassifier(random_state=42, eval_metric='mlogloss'),\n",
        "        param_grid,\n",
        "        n_iter=20,\n",
        "        cv=5,\n",
        "        scoring='accuracy',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    print(\"   - Fitting RandomizedSearchCV...\")\n",
        "    xgb_tuned.fit(X_train, y_train)\n",
        "    \n",
        "    best_params = xgb_tuned.best_params_\n",
        "    best_score = xgb_tuned.best_score_\n",
        "    \n",
        "    print(f\"   ‚úÖ Best parameters: {best_params}\")\n",
        "    print(f\"   ‚úÖ Best CV score: {best_score:.4f}\")\n",
        "    \n",
        "    y_pred_tuned = xgb_tuned.predict(X_test)\n",
        "    tuned_accuracy = accuracy_score(y_test, y_pred_tuned)\n",
        "    print(f\"   ‚úÖ Tuned model test accuracy: {tuned_accuracy:.4f}\")\n",
        "    \n",
        "    best_model = xgb_tuned.best_estimator_\n",
        "    \n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è Hyperparameter tuning not implemented for {best_model_name}\")\n",
        "    best_model = trained_models[best_model_name]\n",
        "    tuned_accuracy = results_df.loc[best_model_name, 'Accuracy']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Final Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final evaluation of the best model\n",
        "print(f\"üéØ Final Evaluation of {best_model_name}\")\n",
        "\n",
        "# Make predictions with the best model\n",
        "y_pred_final = best_model.predict(X_test)\n",
        "y_pred_proba_final = best_model.predict_proba(X_test)\n",
        "\n",
        "# Calculate final metrics\n",
        "final_accuracy = accuracy_score(y_test, y_pred_final)\n",
        "final_precision = precision_score(y_test, y_pred_final, average='weighted')\n",
        "final_recall = recall_score(y_test, y_pred_final, average='weighted')\n",
        "final_f1 = f1_score(y_test, y_pred_final, average='weighted')\n",
        "\n",
        "print(f\"\\nüìä Final Model Performance:\")\n",
        "print(f\"   - Accuracy: {final_accuracy:.4f}\")\n",
        "print(f\"   - Precision: {final_precision:.4f}\")\n",
        "print(f\"   - Recall: {final_recall:.4f}\")\n",
        "print(f\"   - F1-Score: {final_f1:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred_final)\n",
        "print(f\"\\nüîç Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Classification Report\n",
        "print(f\"\\nüìã Detailed Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_final, target_names=['Death (D)', 'Censored (C)', 'Censored-Liver (CL)']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix Heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Death (D)', 'Censored (C)', 'Censored-Liver (CL)'],\n",
        "            yticklabels=['Death (D)', 'Censored (C)', 'Censored-Liver (CL)'])\n",
        "plt.title('üî• Confusion Matrix Heatmap', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Feature Importance (for tree-based models)\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    print(f\"\\nüå≥ Feature Importance Analysis:\")\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': X.columns,\n",
        "        'importance': best_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    print(feature_importance.head(10))\n",
        "    \n",
        "    # Plot feature importance\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    top_features = feature_importance.head(15)\n",
        "    sns.barplot(data=top_features, x='importance', y='feature', palette='viridis')\n",
        "    plt.title('üå≥ Top 15 Feature Importance', fontsize=16, fontweight='bold')\n",
        "    plt.xlabel('Importance')\n",
        "    plt.ylabel('Features')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è Feature importance not available for {best_model_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the final model and scaler\n",
        "print(f\"\\nüíæ Saving the final model...\")\n",
        "\n",
        "# Save the model\n",
        "joblib.dump(best_model, 'cirrhosis_model.pkl')\n",
        "print(\"   ‚úÖ Model saved as 'cirrhosis_model.pkl'\")\n",
        "\n",
        "# Save the scaler\n",
        "joblib.dump(scaler, 'cirrhosis_scaler.pkl')\n",
        "print(\"   ‚úÖ Scaler saved as 'cirrhosis_scaler.pkl'\")\n",
        "\n",
        "# Save feature names for later use\n",
        "feature_names = list(X.columns)\n",
        "joblib.dump(feature_names, 'feature_names.pkl')\n",
        "print(\"   ‚úÖ Feature names saved as 'feature_names.pkl'\")\n",
        "\n",
        "print(f\"\\nüéâ Model training and evaluation complete!\")\n",
        "print(f\"   - Best model: {best_model_name}\")\n",
        "print(f\"   - Final accuracy: {final_accuracy:.4f}\")\n",
        "print(f\"   - Model saved as: cirrhosis_model.pkl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÆ Prediction Demo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_status(input_data):\n",
        "    \"\"\"\n",
        "    Predict survival status for new patient data\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    input_data : dict or pd.DataFrame\n",
        "        Patient clinical features\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    prediction : str\n",
        "        Predicted status (Death, Censored, or Censored-Liver)\n",
        "    probability : array\n",
        "        Prediction probabilities for each class\n",
        "    \"\"\"\n",
        "    # Load the saved model, scaler, and feature names\n",
        "    model = joblib.load('cirrhosis_model.pkl')\n",
        "    scaler = joblib.load('cirrhosis_scaler.pkl')\n",
        "    features = joblib.load('feature_names.pkl')\n",
        "    \n",
        "    # Convert input to DataFrame if dict\n",
        "    if isinstance(input_data, dict):\n",
        "        input_df = pd.DataFrame([input_data])\n",
        "    else:\n",
        "        input_df = input_data.copy()\n",
        "    \n",
        "    # Ensure all required features are present\n",
        "    for feature in features:\n",
        "        if feature not in input_df.columns:\n",
        "            input_df[feature] = 0\n",
        "    \n",
        "    # Select and order features correctly\n",
        "    input_df = input_df[features]\n",
        "    \n",
        "    # Scale the input\n",
        "    input_scaled = scaler.transform(input_df)\n",
        "    \n",
        "    # Make prediction\n",
        "    prediction = model.predict(input_scaled)\n",
        "    probabilities = model.predict_proba(input_scaled)\n",
        "    \n",
        "    # Map prediction to label\n",
        "    status_map = {0: 'Death (D)', 1: 'Censored (C)', 2: 'Censored-Liver (CL)'}\n",
        "    predicted_status = status_map[prediction[0]]\n",
        "    \n",
        "    return predicted_status, probabilities[0]\n",
        "\n",
        "print(\"‚úÖ Prediction function created successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Make a prediction on a sample from test set\n",
        "print(\"üîÆ Testing the prediction function...\")\n",
        "\n",
        "# Get a random sample from test set\n",
        "sample_idx = np.random.randint(0, len(X_test))\n",
        "sample_data = X_test[sample_idx:sample_idx+1]\n",
        "\n",
        "# Convert to DataFrame\n",
        "sample_df = pd.DataFrame(sample_data, columns=X.columns)\n",
        "\n",
        "# Make prediction\n",
        "predicted_status, probabilities = predict_status(sample_df)\n",
        "\n",
        "print(f\"\\nüìä Sample Patient Prediction:\")\n",
        "print(f\"   - Predicted Status: {predicted_status}\")\n",
        "print(f\"   - Prediction Probabilities:\")\n",
        "print(f\"     ‚Ä¢ Death (D): {probabilities[0]:.2%}\")\n",
        "print(f\"     ‚Ä¢ Censored (C): {probabilities[1]:.2%}\")\n",
        "print(f\"     ‚Ä¢ Censored-Liver (CL): {probabilities[2]:.2%}\")\n",
        "print(f\"   - Actual Status: {['Death (D)', 'Censored (C)', 'Censored-Liver (CL)'][y_test.iloc[sample_idx]]}\")\n",
        "\n",
        "# Show confidence\n",
        "confidence = np.max(probabilities)\n",
        "print(f\"   - Confidence: {confidence:.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé® Optional: Streamlit UI Code\n",
        "\n",
        "Below is a code snippet for creating an interactive web application using Streamlit:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save this as 'app.py' and run with: streamlit run app.py\n",
        "\n",
        "streamlit_code = '''\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# Load model, scaler, and features\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    model = joblib.load('cirrhosis_model.pkl')\n",
        "    scaler = joblib.load('cirrhosis_scaler.pkl')\n",
        "    features = joblib.load('feature_names.pkl')\n",
        "    return model, scaler, features\n",
        "\n",
        "model, scaler, features = load_model()\n",
        "\n",
        "# Title\n",
        "st.title('ü©∫ Cirrhosis Patient Survival Prediction')\n",
        "st.write('Predict patient survival status based on clinical features')\n",
        "\n",
        "# Sidebar inputs\n",
        "st.sidebar.header('Patient Information')\n",
        "\n",
        "# Numeric inputs\n",
        "age = st.sidebar.number_input('Age', min_value=20, max_value=100, value=50)\n",
        "bilirubin = st.sidebar.number_input('Bilirubin', min_value=0.0, max_value=30.0, value=1.0)\n",
        "albumin = st.sidebar.number_input('Albumin', min_value=0.0, max_value=6.0, value=3.5)\n",
        "prothrombin = st.sidebar.number_input('Prothrombin', min_value=5.0, max_value=20.0, value=10.0)\n",
        "cholesterol = st.sidebar.number_input('Cholesterol', min_value=50.0, max_value=600.0, value=200.0)\n",
        "\n",
        "# Categorical inputs\n",
        "drug = st.sidebar.selectbox('Drug', ['D-penicillamine', 'Placebo'])\n",
        "sex = st.sidebar.selectbox('Sex', ['M', 'F'])\n",
        "ascites = st.sidebar.selectbox('Ascites', ['Y', 'N'])\n",
        "\n",
        "# Predict button\n",
        "if st.sidebar.button('Predict'):\n",
        "    # Create input dataframe (simplified - adjust based on actual features)\n",
        "    input_data = pd.DataFrame([[0] * len(features)], columns=features)\n",
        "    \n",
        "    # Fill in the known values (this is simplified)\n",
        "    # You'll need to map the inputs to the correct one-hot encoded features\n",
        "    \n",
        "    # Scale and predict\n",
        "    input_scaled = scaler.transform(input_data)\n",
        "    prediction = model.predict(input_scaled)\n",
        "    probabilities = model.predict_proba(input_scaled)[0]\n",
        "    \n",
        "    # Display results\n",
        "    status_map = {0: 'Death (D)', 1: 'Censored (C)', 2: 'Censored-Liver (CL)'}\n",
        "    st.success(f'Predicted Status: {status_map[prediction[0]]}')\n",
        "    \n",
        "    st.write('Prediction Probabilities:')\n",
        "    st.bar_chart({'Death': probabilities[0], 'Censored': probabilities[1], 'Censored-Liver': probabilities[2]})\n",
        "'''\n",
        "\n",
        "print(\"üìù Streamlit code snippet created!\")\n",
        "print(\"\\nTo use this code:\")\n",
        "print(\"1. Save it as 'app.py'\")\n",
        "print(\"2. Install streamlit: pip install streamlit\")\n",
        "print(\"3. Run: streamlit run app.py\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
